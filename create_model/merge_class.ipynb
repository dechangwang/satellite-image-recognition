{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yokoyang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D, Cropping2D\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import keras\n",
    "import h5py\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import History\n",
    "import pandas as pd\n",
    "from keras.backend import binary_crossentropy\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import random\n",
    "import threading\n",
    "\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows = 112\n",
    "img_cols = 112\n",
    "\n",
    "smooth = 1e-12\n",
    "\n",
    "num_channels = 3\n",
    "num_mask_channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coef(y_true, y_pred):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred, axis=[0, -1, -2])\n",
    "\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_int(y_true, y_pred):\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "\n",
    "    intersection = K.sum(y_true * y_pred_pos, axis=[0, -1, -2])\n",
    "    sum_ = K.sum(y_true + y_pred_pos, axis=[0, -1, -2])\n",
    "\n",
    "    jac = (intersection + smooth) / (sum_ - intersection + smooth)\n",
    "\n",
    "    return K.mean(jac)\n",
    "\n",
    "\n",
    "def jaccard_coef_loss(y_true, y_pred):\n",
    "    return -K.log(jaccard_coef(y_true, y_pred)) + binary_crossentropy(y_pred, y_true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet0():\n",
    "    inputs = Input((num_channels, img_rows, img_cols))\n",
    "    conv1 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(inputs)\n",
    "    conv1 = BatchNormalization(mode=0, axis=1)(conv1)\n",
    "    conv1 = keras.layers.advanced_activations.ELU()(conv1)\n",
    "    conv1 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(conv1)\n",
    "    conv1 = BatchNormalization(mode=0, axis=1)(conv1)\n",
    "    conv1 = keras.layers.advanced_activations.ELU()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Convolution2D(64, 3, 3, border_mode='same', init='he_uniform')(pool1)\n",
    "    conv2 = BatchNormalization(mode=0, axis=1)(conv2)\n",
    "    conv2 = keras.layers.advanced_activations.ELU()(conv2)\n",
    "    conv2 = Convolution2D(64, 3, 3, border_mode='same', init='he_uniform')(conv2)\n",
    "    conv2 = BatchNormalization(mode=0, axis=1)(conv2)\n",
    "    conv2 = keras.layers.advanced_activations.ELU()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Convolution2D(128, 3, 3, border_mode='same', init='he_uniform')(pool2)\n",
    "    conv3 = BatchNormalization(mode=0, axis=1)(conv3)\n",
    "    conv3 = keras.layers.advanced_activations.ELU()(conv3)\n",
    "    conv3 = Convolution2D(128, 3, 3, border_mode='same', init='he_uniform')(conv3)\n",
    "    conv3 = BatchNormalization(mode=0, axis=1)(conv3)\n",
    "    conv3 = keras.layers.advanced_activations.ELU()(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Convolution2D(256, 3, 3, border_mode='same', init='he_uniform')(pool3)\n",
    "    conv4 = BatchNormalization(mode=0, axis=1)(conv4)\n",
    "    conv4 = keras.layers.advanced_activations.ELU()(conv4)\n",
    "    conv4 = Convolution2D(256, 3, 3, border_mode='same', init='he_uniform')(conv4)\n",
    "    conv4 = BatchNormalization(mode=0, axis=1)(conv4)\n",
    "    conv4 = keras.layers.advanced_activations.ELU()(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Convolution2D(512, 3, 3, border_mode='same', init='he_uniform')(pool4)\n",
    "    conv5 = BatchNormalization(mode=0, axis=1)(conv5)\n",
    "    conv5 = keras.layers.advanced_activations.ELU()(conv5)\n",
    "    conv5 = Convolution2D(512, 3, 3, border_mode='same', init='he_uniform')(conv5)\n",
    "    conv5 = BatchNormalization(mode=0, axis=1)(conv5)\n",
    "    conv5 = keras.layers.advanced_activations.ELU()(conv5)\n",
    "\n",
    "    up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Convolution2D(256, 3, 3, border_mode='same', init='he_uniform')(up6)\n",
    "    conv6 = BatchNormalization(mode=0, axis=1)(conv6)\n",
    "    conv6 = keras.layers.advanced_activations.ELU()(conv6)\n",
    "    conv6 = Convolution2D(256, 3, 3, border_mode='same', init='he_uniform')(conv6)\n",
    "    conv6 = BatchNormalization(mode=0, axis=1)(conv6)\n",
    "    conv6 = keras.layers.advanced_activations.ELU()(conv6)\n",
    "\n",
    "    up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Convolution2D(128, 3, 3, border_mode='same', init='he_uniform')(up7)\n",
    "    conv7 = BatchNormalization(mode=0, axis=1)(conv7)\n",
    "    conv7 = keras.layers.advanced_activations.ELU()(conv7)\n",
    "    conv7 = Convolution2D(128, 3, 3, border_mode='same', init='he_uniform')(conv7)\n",
    "    conv7 = BatchNormalization(mode=0, axis=1)(conv7)\n",
    "    conv7 = keras.layers.advanced_activations.ELU()(conv7)\n",
    "\n",
    "    up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Convolution2D(64, 3, 3, border_mode='same', init='he_uniform')(up8)\n",
    "    conv8 = BatchNormalization(mode=0, axis=1)(conv8)\n",
    "    conv8 = keras.layers.advanced_activations.ELU()(conv8)\n",
    "    conv8 = Convolution2D(64, 3, 3, border_mode='same', init='he_uniform')(conv8)\n",
    "    conv8 = BatchNormalization(mode=0, axis=1)(conv8)\n",
    "    conv8 = keras.layers.advanced_activations.ELU()(conv8)\n",
    "\n",
    "    up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(up9)\n",
    "    conv9 = BatchNormalization(mode=0, axis=1)(conv9)\n",
    "    conv9 = keras.layers.advanced_activations.ELU()(conv9)\n",
    "    conv9 = Convolution2D(32, 3, 3, border_mode='same', init='he_uniform')(conv9)\n",
    "    crop9 = Cropping2D(cropping=((16, 16), (16, 16)))(conv9)\n",
    "    conv9 = BatchNormalization(mode=0, axis=1)(crop9)\n",
    "    conv9 = keras.layers.advanced_activations.ELU()(conv9)\n",
    "    conv10 = Convolution2D(1, 1, num_mask_channels, activation='sigmoid')(conv9)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_axis(x, axis):\n",
    "    x = np.asarray(x).swapaxes(axis, 0)\n",
    "    x = x[::-1, ...]\n",
    "    x = x.swapaxes(0, axis)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_batch(X, y, batch_size):\n",
    "    X_batch = np.zeros((batch_size, num_channels, img_rows, img_cols))\n",
    "    y_batch = np.zeros((batch_size, num_mask_channels, img_rows, img_cols))\n",
    "    X_height = X.shape[2]\n",
    "    X_width = X.shape[3]\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        random_width = random.randint(0, X_width - img_cols - 1)\n",
    "        random_height = random.randint(0, X_height - img_rows - 1)\n",
    "\n",
    "        random_image = random.randint(0, X.shape[0] - 1)\n",
    "\n",
    "        y_batch[i] = y[random_image, :, random_height: random_height + img_rows, random_width: random_width + img_cols]\n",
    "        X_batch[i] = np.array(\n",
    "            X[random_image, :, random_height: random_height + img_rows, random_width: random_width + img_cols])\n",
    "    return X_batch, y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class threadsafe_iter:\n",
    "    \"\"\"Takes an iterator/generator and makes it thread-safe by\n",
    "    serializing call to the `next` method of given iterator/generator.\n",
    "    \"\"\"\n",
    "    def __init__(self, it):\n",
    "        self.it = it\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def next(self):\n",
    "        with self.lock:\n",
    "            return self.it.next()\n",
    "\n",
    "\n",
    "def threadsafe_generator(f):\n",
    "    \"\"\"A decorator that takes a generator function and makes it thread-safe.\n",
    "    \"\"\"\n",
    "    def g(*a, **kw):\n",
    "        return threadsafe_iter(f(*a, **kw))\n",
    "    return g\n",
    "\n",
    "\n",
    "@threadsafe_generator\n",
    "def batch_generator(X, y, batch_size, horizontal_flip=False, vertical_flip=False, swap_axis=False):\n",
    "    while True:\n",
    "        X_batch, y_batch = form_batch(X, y, batch_size)\n",
    "\n",
    "        for i in range(X_batch.shape[0]):\n",
    "            xb = X_batch[i]\n",
    "            yb = y_batch[i]\n",
    "\n",
    "            if horizontal_flip:\n",
    "                if np.random.random() < 0.5:\n",
    "                    xb = flip_axis(xb, 1)\n",
    "                    yb = flip_axis(yb, 1)\n",
    "\n",
    "            if vertical_flip:\n",
    "                if np.random.random() < 0.5:\n",
    "                    xb = flip_axis(xb, 2)\n",
    "                    yb = flip_axis(yb, 2)\n",
    "\n",
    "            if swap_axis:\n",
    "                if np.random.random() < 0.5:\n",
    "                    xb = xb.swapaxes(1, 2)\n",
    "                    yb = yb.swapaxes(1, 2)\n",
    "\n",
    "            X_batch[i] = xb\n",
    "            y_batch[i] = yb\n",
    "\n",
    "        yield X_batch, y_batch[:, :, 16:16 + img_rows - 32, 16:16 + img_cols - 32]\n",
    "\n",
    "\n",
    "def save_model(model, cross):\n",
    "    json_string = model.to_json()\n",
    "    if not os.path.isdir('cache'):\n",
    "        os.mkdir('cache')\n",
    "    json_name = 'architecture_' + cross + '.json'\n",
    "    weight_name = 'model_weights_' + cross + '.h5'\n",
    "    open(os.path.join('cache', json_name), 'w').write(json_string)\n",
    "    model.save_weights(os.path.join('cache', weight_name), overwrite=True)\n",
    "\n",
    "\n",
    "def save_history(history, suffix):\n",
    "    filename = 'history/history_' + suffix + '.csv'\n",
    "    pd.DataFrame(history.history).to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "def read_model(cross=''):\n",
    "    json_name = 'architecture_' + cross + '.json'\n",
    "    weight_name = 'model_weights_' + cross + '.h5'\n",
    "    model = model_from_json(open(os.path.join('../src/cache', json_name)).read())\n",
    "    model.load_weights(os.path.join('../src/cache', weight_name))\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data'\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "print('[{}] Creating and compiling model...'.format(str(datetime.datetime.now())))\n",
    "\n",
    "model = get_unet0()\n",
    "\n",
    "print('[{}] Reading train...'.format(str(datetime.datetime.now())))\n",
    "f = h5py.File(os.path.join(data_path, 'train_16.h5'), 'r')\n",
    "\n",
    "X_train = f['train']\n",
    "\n",
    "y_train = np.array(f['train_mask'])[:, 4]\n",
    "y_train = np.expand_dims(y_train, 1)\n",
    "print(y_train.shape)\n",
    "\n",
    "train_ids = np.array(f['train_ids'])\n",
    "\n",
    "batch_size = 128\n",
    "nb_epoch = 50\n",
    "\n",
    "history = History()\n",
    "callbacks = [\n",
    "    history,\n",
    "]\n",
    "\n",
    "suffix = 'trees_2_'\n",
    "model.compile(optimizer=Nadam(lr=1e-2), loss=jaccard_coef_loss, metrics=['binary_crossentropy', jaccard_coef_int])\n",
    "model.fit_generator(\n",
    "    batch_generator(X_train, y_train, batch_size, horizontal_flip=True, vertical_flip=True, swap_axis=True),\n",
    "    nb_epoch=nb_epoch,\n",
    "    verbose=1,\n",
    "    samples_per_epoch=batch_size * 400,\n",
    "    callbacks=callbacks,\n",
    "    nb_worker=8\n",
    "    )\n",
    "\n",
    "save_model(model, \"{batch}_{epoch}_{suffix}\".format(batch=batch_size, epoch=nb_epoch, suffix=suffix))\n",
    "save_history(history, suffix)\n",
    "f.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
